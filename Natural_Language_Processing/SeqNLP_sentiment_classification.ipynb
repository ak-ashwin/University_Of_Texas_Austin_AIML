{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "# Sentiment Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq4RCyyPSYRp"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WIuDIfr1P0in"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "he32BWZNP0-3",
    "outputId": "cbb6d498-58bb-4152-994e-ed02ee4da41d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGCtiXUhSWss"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models, regularizers, layers, optimizers, losses, metrics\n",
    "\n",
    "vocab_size = 10000 #vocab size\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCPC_WN-eCyw"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "vocab_size = 10000 #vocab size\n",
    "maxlen = 300  #number of word used from each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMEsHYrWxdtk"
   },
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0g381XzeCyz"
   },
   "outputs": [],
   "source": [
    "#load dataset as a list of ints\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "#make all sequences of the same length\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test =  pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1IXOrBFD3fi"
   },
   "source": [
    "Inference: From IMDB dataset, vocab size defined 10000 and loaded the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "bAtcjaghlxe-",
    "outputId": "b9af2833-0a16-4731-cc30-8295875a945e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    1\n",
      "  194 1153  194 8255   78  228    5    6 1463 4369 5012  134   26    4\n",
      "  715    8  118 1634   14  394   20   13  119  954  189  102    5  207\n",
      "  110 3103   21   14   69  188    8   30   23    7    4  249  126   93\n",
      "    4  114    9 2300 1523    5  647    4  116    9   35 8163    4  229\n",
      "    9  340 1322    4  118    9    4  130 4901   19    4 1002    5   89\n",
      "   29  952   46   37    4  455    9   45   43   38 1543 1905  398    4\n",
      " 1649   26 6853    5  163   11 3215    2    4 1153    9  194  775    7\n",
      " 8255    2  349 2637  148  605    2 8003   15  123  125   68    2 6853\n",
      "   15  349  165 4362   98    5    4  228    9   43    2 1157   15  299\n",
      "  120    5  120  174   11  220  175  136   50    9 4373  228 8255    5\n",
      "    2  656  245 2350    5    4 9837  131  152  491   18    2   32 7464\n",
      " 1212   14    9    6  371   78   22  625   64 1382    9    8  168  145\n",
      "   23    4 1690   15   16    4 1355    5   28    6   52  154  462   33\n",
      "   89   78  285   16  145   95]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(x_train[1])\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKJ-WQ8wEA9z"
   },
   "source": [
    "Inference: Provided data is already pre-processed, means convered into numbers. Instance of x_train[1] checked with its sentiment lable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Yj9aaRDVzmRz",
    "outputId": "aedee014-9e5b-46db-d30e-d3d39fbd42af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "a = (y_train == 0).sum()\n",
    "print(a)\n",
    "b = (y_train == 1).sum()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "F0jCITA41tze",
    "outputId": "efe22d80-893d-4807-cf6c-d33e74c9e3f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "a = (y_test == 0).sum()\n",
    "print(a)\n",
    "b = (y_test == 1).sum()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGo_Xwt9D09O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98X47NreE339"
   },
   "source": [
    "Inference: As all the sentence has non-uniform lenghts, to make it uniform to maximum lenght 300 words pad_sequence introduced and dataset split in to x_train and x_test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "boYn_T3RgGKz"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Jy6n-uM2eCy2",
    "outputId": "ae17546f-0326-4f57-fcee-e46e2f8cf5d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n",
      "(25000, 300)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dybtUgUReCy8"
   },
   "source": [
    "## Build Keras Embedding Layer Model\n",
    "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
    "\n",
    "* The embedding layer can be used at the start of a larger deep learning model. \n",
    "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
    "* Use the embedding layer to train our own word2vec models.\n",
    "\n",
    "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A5OLM4eBeCy9",
    "outputId": "832a3368-2b15-471c-d7ae-f24c801d2e54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 238,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/drscarlat/imdb-sentiment-analysis-keras-and-tensorflow\n",
    "\n",
    "# Get the word index and then Create a key-value pair for word and word_id (12.5 points)\n",
    "\n",
    "words = imdb.get_word_index()\n",
    "type(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4nDSqR_yTh7D",
    "outputId": "eac568c5-21b0-464b-b792-beddb9959630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wednesday\n",
      "16818\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# printing a sample key and value from the words dictionary\n",
    "print(list(words.keys())[35])\n",
    "print(list(words.values())[35])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "NAwqpeefTlNx",
    "outputId": "847004aa-9a24-4e0b-d5aa-537c22c6571c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\ba : 68893\n",
      "\u0010own : 70879\n",
      "' : 755\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# printing 1st 3 set with sorted key\n",
    "for k in list(sorted(words.keys()))[0:3]:\n",
    "  print (k,':',words[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "akRukdEHTqH3",
    "outputId": "38aa3d79-b74e-4b6a-a204-1ca0fc8e7d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn : 34701\n",
      "tsukino : 52006\n",
      "nunnery : 52007\n",
      "sonja : 16816\n",
      "vani : 63951\n",
      "woods : 1408\n",
      "spiders : 16115\n",
      "hanging : 2345\n",
      "woody : 2289\n",
      "trawling : 52008\n",
      "hold's : 52009\n",
      "comically : 11307\n",
      "localized : 40830\n",
      "disobeying : 30568\n",
      "'royale : 52010\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# printing top 15 key:value set from the dictionary\n",
    "for k in list(words)[0:15]:\n",
    "  print(k,':',words[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "5kbWRLwYTrlz",
    "outputId": "b34bece2-4ba3-4ae3-ca3c-a6a1739b217c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "1277\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# code to decode the comment\n",
    "reverse_word_index = dict(\n",
    "[(value, key) for (key, value) in words.items()])\n",
    "\n",
    "decoded_review = ' '.join(\n",
    "[reverse_word_index.get(i - 3, '?') for i in x_train[0]])\n",
    "\n",
    "print(decoded_review)\n",
    "print(len(decoded_review))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SIVJeTHeVZ6i"
   },
   "outputs": [],
   "source": [
    "def cut_words(records):\n",
    "    for lines in records:\n",
    "        for word in lines:\n",
    "            word = word[0:50]\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "8c-Vmu2nVlRE",
    "outputId": "806f7c5b-3f1e-4502-a069-edb2140e659f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "print(cut_words(decoded_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "5cdFokEbD-mm",
    "outputId": "6a31159b-7de3-46e4-f2aa-b2df0b2c3166"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    1,   14,   22,   16,   43,  530,\n",
       "        973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,  173,   36,\n",
       "        256,    5,   25,  100,   43,  838,  112,   50,  670,    2,    9,\n",
       "         35,  480,  284,    5,  150,    4,  172,  112,  167,    2,  336,\n",
       "        385,   39,    4,  172, 4536, 1111,   17,  546,   38,   13,  447,\n",
       "          4,  192,   50,   16,    6,  147, 2025,   19,   14,   22,    4,\n",
       "       1920, 4613,  469,    4,   22,   71,   87,   12,   16,   43,  530,\n",
       "         38,   76,   15,   13, 1247,    4,   22,   17,  515,   17,   12,\n",
       "         16,  626,   18,    2,    5,   62,  386,   12,    8,  316,    8,\n",
       "        106,    5,    4, 2223, 5244,   16,  480,   66, 3785,   33,    4,\n",
       "        130,   12,   16,   38,  619,    5,   25,  124,   51,   36,  135,\n",
       "         48,   25, 1415,   33,    6,   22,   12,  215,   28,   77,   52,\n",
       "          5,   14,  407,   16,   82,    2,    8,    4,  107,  117, 5952,\n",
       "         15,  256,    4,    2,    7, 3766,    5,  723,   36,   71,   43,\n",
       "        530,  476,   26,  400,  317,   46,    7,    4,    2, 1029,   13,\n",
       "        104,   88,    4,  381,   15,  297,   98,   32, 2071,   56,   26,\n",
       "        141,    6,  194, 7486,   18,    4,  226,   22,   21,  134,  476,\n",
       "         26,  480,    5,  144,   30, 5535,   18,   51,   36,   28,  224,\n",
       "         92,   25,  104,    4,  226,   65,   16,   38, 1334,   88,   12,\n",
       "         16,  283,    5,   16, 4472,  113,  103,   32,   15,   16, 5345,\n",
       "         19,  178,   32], dtype=int32)"
      ]
     },
     "execution_count": 245,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "JbjacHLKLMne",
    "outputId": "0f8f8275-c90c-46c4-b0b9-ef842a12839d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---review with words---\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'the', 'effort', 'still', 'been', 'that', 'usually', 'makes', 'for', 'of', 'finished', 'sucking', 'ended', 'and', 'an', 'because', 'before', 'if', 'just', 'though', 'something', 'know', 'novel', 'female', 'i', 'i', 'slowly', 'lot', 'of', 'above', 'and', 'with', 'connect', 'in', 'of', 'script', 'their', 'that', 'out', 'end', 'his', 'and', 'i', 'i']\n",
      "---label---\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in x_train[5]])\n",
    "print('---label---')\n",
    "print(y_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpV__0f_Wl1J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import models, regularizers, layers, optimizers, losses, metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(100, kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(300,)))\n",
    "model.add(layers.Dropout(0.50))\n",
    "model.add(layers.Dense(50, kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n",
    "model.add(layers.Dropout(0.50))\n",
    "model.add(layers.Dense(30, kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n",
    "model.add(layers.Dropout(0.50))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n",
    "model.add(layers.Dropout(0.50))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "kFcR0Fm2WrZi",
    "outputId": "200b87df-f987-4586-dfbb-7be6a239eea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                496       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 37,193\n",
      "Trainable params: 37,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "8qwYsECHWs8C",
    "outputId": "86871610-11b8-42f4-915d-624b1a9dca04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_val  (5000, 300)\n",
      "partial_x_train  (20000, 300)\n",
      "y_val  (5000,)\n",
      "partial_y_train  (20000,)\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 9.4219 - acc: 0.4994 - val_loss: 9.6470 - val_acc: 0.4908\n",
      "Epoch 2/20\n",
      " - 3s - loss: 9.3483 - acc: 0.5028 - val_loss: 9.6660 - val_acc: 0.4908\n",
      "Epoch 3/20\n",
      " - 3s - loss: 9.2947 - acc: 0.5024 - val_loss: 9.6926 - val_acc: 0.4908\n",
      "Epoch 4/20\n",
      " - 3s - loss: 9.1300 - acc: 0.4979 - val_loss: 9.6670 - val_acc: 0.4908\n",
      "Epoch 5/20\n",
      " - 3s - loss: 8.7823 - acc: 0.5072 - val_loss: 9.7155 - val_acc: 0.4908\n",
      "Epoch 6/20\n",
      " - 3s - loss: 8.6528 - acc: 0.4995 - val_loss: 9.7058 - val_acc: 0.4908\n",
      "Epoch 7/20\n",
      " - 3s - loss: 8.2892 - acc: 0.5010 - val_loss: 9.6797 - val_acc: 0.4908\n",
      "Epoch 8/20\n",
      " - 3s - loss: 7.6600 - acc: 0.5026 - val_loss: 9.6461 - val_acc: 0.4908\n",
      "Epoch 9/20\n",
      " - 3s - loss: 7.3093 - acc: 0.4982 - val_loss: 9.6253 - val_acc: 0.4908\n",
      "Epoch 10/20\n",
      " - 3s - loss: 6.9079 - acc: 0.4985 - val_loss: 9.5562 - val_acc: 0.4908\n",
      "Epoch 11/20\n",
      " - 3s - loss: 5.9467 - acc: 0.4978 - val_loss: 2.0556 - val_acc: 0.4908\n",
      "Epoch 12/20\n",
      " - 3s - loss: 4.2506 - acc: 0.5029 - val_loss: 1.9664 - val_acc: 0.4908\n",
      "Epoch 13/20\n",
      " - 3s - loss: 3.6713 - acc: 0.4990 - val_loss: 1.9123 - val_acc: 0.4908\n",
      "Epoch 14/20\n",
      " - 3s - loss: 3.1958 - acc: 0.5016 - val_loss: 1.8379 - val_acc: 0.4908\n",
      "Epoch 15/20\n",
      " - 3s - loss: 2.9879 - acc: 0.5002 - val_loss: 1.7949 - val_acc: 0.4908\n",
      "Epoch 16/20\n",
      " - 3s - loss: 2.7909 - acc: 0.5005 - val_loss: 1.7066 - val_acc: 0.4908\n",
      "Epoch 17/20\n",
      " - 3s - loss: 2.7098 - acc: 0.5022 - val_loss: 1.6191 - val_acc: 0.4908\n",
      "Epoch 18/20\n",
      " - 3s - loss: 2.4478 - acc: 0.5036 - val_loss: 1.5571 - val_acc: 0.4908\n",
      "Epoch 19/20\n",
      " - 3s - loss: 2.2069 - acc: 0.5028 - val_loss: 1.4296 - val_acc: 0.4908\n",
      "Epoch 20/20\n",
      " - 3s - loss: 2.0456 - acc: 0.5010 - val_loss: 1.5116 - val_acc: 0.4908\n",
      "25000/25000 [==============================] - 1s 48us/step\n",
      "____________________________________________________________________________________________________\n",
      "Test Loss and Accuracy\n",
      "results  [1.5115882901763915, 0.5]\n"
     ]
    }
   ],
   "source": [
    "x_val = x_train[:5000]\n",
    "x_train_m = x_train[5000:]\n",
    "y_val = y_train[:5000]\n",
    "y_train_m = y_train[5000:]\n",
    "\n",
    "print(\"x_val \", x_val.shape)\n",
    "print(\"partial_x_train \", x_train_m.shape)\n",
    "print(\"y_val \", y_val.shape)\n",
    "print(\"partial_y_train \", y_train_m.shape)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train_m, y_train_m, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val), verbose=2)\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(\"_\"*100)\n",
    "print(\"Test Loss and Accuracy\")\n",
    "print(\"results \", results)\n",
    "# history_dict = history.history\n",
    "# history_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "_aTZJ1-jD-mr",
    "outputId": "a70b923b-b9e1-47c4-d7e5-df14a86be7da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 300, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 130)               84760     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 131       \n",
      "=================================================================\n",
      "Total params: 404,891\n",
      "Trainable params: 404,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 220s 9ms/step - loss: 0.5281 - acc: 0.7404 - val_loss: 0.4773 - val_acc: 0.7847\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 215s 9ms/step - loss: 0.3244 - acc: 0.8673 - val_loss: 0.3608 - val_acc: 0.8473\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 215s 9ms/step - loss: 0.2494 - acc: 0.9065 - val_loss: 0.4349 - val_acc: 0.8116\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 216s 9ms/step - loss: 0.2108 - acc: 0.9206 - val_loss: 0.3702 - val_acc: 0.8362\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 218s 9ms/step - loss: 0.1727 - acc: 0.9384 - val_loss: 0.3794 - val_acc: 0.8604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f205a0d3e80>"
      ]
     },
     "execution_count": 250,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# above defined model is not able to reduce the val_loss.\n",
    "# So, Trying with LSTM now\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vecor_length, input_length=maxlen))\n",
    "model.add(LSTM(130))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcZwZe1BJUxA"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "path= '/content/drive/My Drive/NLP/'\n",
    "filename = path+'seqNLP1.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKJpjHdgMC2v"
   },
   "outputs": [],
   "source": [
    "\n",
    "# load the model from disk\n",
    "# model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YzxyuKpPD-mu",
    "outputId": "6cdcca44-6432-48d4-af46-9a08180f2cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.04%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uicZaD0Sw6MF"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "WYYxxq3Zz1xh",
    "outputId": "ad8beb5a-98d6-4f11-d4b0-01fbbef74b17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06303275],\n",
       "       [0.9965552 ],\n",
       "       [0.92713594],\n",
       "       ...,\n",
       "       [0.0446955 ],\n",
       "       [0.02282029],\n",
       "       [0.96978694]], dtype=float32)"
      ]
     },
     "execution_count": 255,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TxNDNhrseCzA",
    "outputId": "28ad1185-42d1-4f45-db3c-ff3c55770c8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3794466183996201, 0.8604]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "9iGXH76JOJkn",
    "outputId": "e69b579b-bff1-4c3e-b253-6b7c9d15bb43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3CSVVPPeCzD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igq8Qm8GeCzG"
   },
   "source": [
    "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "0AqOnLa2eCzH",
    "outputId": "af3f3245-fbfc-448b-eaac-f7c0b36ac56e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.86     12500\n",
      "           1       0.84      0.89      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "-dUDSg7VeCzM",
    "outputId": "46c03070-a40c-40f2-cfe0-3c76b00f6ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 embedding_8\n",
      "1 lstm_8\n",
      "2 dense_23\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "for i, layer in enumerate(model.layers):\n",
    "  print(i, layer.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Tskt_1npeCzP",
    "outputId": "e41ee8b0-1399-468a-f451-1c9b73bdeaeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300, 32)\n",
      "(25000, 130)\n",
      "(25000, 1)\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)  # Creates a model that will return these outputs, given the model input\n",
    "\n",
    "activations = activation_model.predict(x_test) \n",
    "\n",
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)\n",
    "\n",
    "second_layer_activation = activations[1]\n",
    "print(second_layer_activation.shape)\n",
    "\n",
    "third_layer_activation = activations[2]\n",
    "print(third_layer_activation.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ap7H-S_2IzDE",
    "outputId": "2ebf2a02-a325-431d-fdf2-3d8845d143d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06303275],\n",
       "       [0.9965552 ],\n",
       "       [0.92713594],\n",
       "       ...,\n",
       "       [0.0446955 ],\n",
       "       [0.02282029],\n",
       "       [0.96978694]], dtype=float32)"
      ]
     },
     "execution_count": 261,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nkn_LKdFTOHy"
   },
   "outputs": [],
   "source": [
    "words = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "2psMYdA5I1BD",
    "outputId": "caf31f31-61cd-47b4-ee04-88cae677ecf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? please give this one a miss br br ? ? and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite ? so all you madison fans give this a miss\n"
     ]
    }
   ],
   "source": [
    "# code to decode the comment from test set\n",
    "reverse_word_index = dict(\n",
    "[(value, key) for (key, value) in words.items()])\n",
    "\n",
    "decoded_review = ' '.join(\n",
    "[reverse_word_index.get(i - 3, '?') for i in x_test[0]])\n",
    "\n",
    "print(decoded_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PCEkMBi8I2-8",
    "outputId": "ae9eebe6-86e4-465c-ebc9-2b5ce304060e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06303275], dtype=float32)"
      ]
     },
     "execution_count": 264,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction from the final layer for the 0th record of test set\n",
    "activations[2][0]\n",
    "\n",
    "# based on np.where(y_pred > 0.5, 1, 0) conversion, the value is < 0.5, hence the prediction is -ve for the comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKlgR08PI5o4"
   },
   "outputs": [],
   "source": [
    "pred = np.where(activations[2] > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "tShWFSd3I7f9",
    "outputId": "63669e16-5d92-40f6-a90b-29f21e5f296a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# displaying Prediction for 1st 5 record from test set\n",
    "for i in range(5):\n",
    "  print(pred[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WkBsRPqOI9PG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFS5D-17aPkG"
   },
   "source": [
    "## Conclusion:\n",
    "\n",
    "LSTM gives higher accuracy for the above project around 86% and can be used for production. LSTM number of hidden layers is 130 which gives the highest accuracy."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SeqNLP_Project1_Questions (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
